{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "134c198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "from arch import arch_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1eb266c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             PL  VaRBHS  VaREWMA  VaRn  VaRt  VaRPot  ESEWMA  ESn  ESt  ESPot  \\\n",
      "Date                                                                            \n",
      "2005-01-01  190     NaN      NaN   NaN   NaN     NaN     NaN  NaN  NaN    NaN   \n",
      "2005-01-02  190     NaN      NaN   NaN   NaN     NaN     NaN  NaN  NaN    NaN   \n",
      "2005-01-03 -220     NaN      NaN   NaN   NaN     NaN     NaN  NaN  NaN    NaN   \n",
      "2005-01-05  120     NaN      NaN   NaN   NaN     NaN     NaN  NaN  NaN    NaN   \n",
      "2005-01-06  450     NaN      NaN   NaN   NaN     NaN     NaN  NaN  NaN    NaN   \n",
      "\n",
      "            losses  \n",
      "Date                \n",
      "2005-01-01    -190  \n",
      "2005-01-02    -190  \n",
      "2005-01-03     220  \n",
      "2005-01-05    -120  \n",
      "2005-01-06    -450  \n"
     ]
    }
   ],
   "source": [
    "# Read CSV file\n",
    "filename='D:\\\\2023semester\\\\Lund University\\\\Financial Risk Management\\\\lab2\\\\DataLab2.xlsx'\n",
    "table = pd.read_excel(filename)\n",
    "table['Date'] = pd.to_datetime(table['Date'])\n",
    "table = table.set_index('Date')\n",
    "table['losses'] = table['PL']*-1\n",
    "estimation_data = table.iloc[0:500]\n",
    "backtesting_sample = table.iloc[500:]\n",
    "data_2007 = table[table.index.year == 2007]\n",
    "data_2008 = table[table.index.year == 2008]\n",
    "print(table.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9cbdfc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: VaRBHS\n",
      "2007 VaR Exceptions: 15 Expected Number: 2.52, P-value: 0.0000, Result: Reject\n",
      "2008 VaR Exceptions: 7 Expected Number: 2.52, P-value: 0.0143, Result: Cannot Reject\n",
      "Model: VaREWMA\n",
      "2007 VaR Exceptions: 5 Expected Number: 2.52, P-value: 0.1105, Result: Cannot Reject\n",
      "2008 VaR Exceptions: 6 Expected Number: 2.52, P-value: 0.0425, Result: Cannot Reject\n",
      "Model: VaRn\n",
      "2007 VaR Exceptions: 36 Expected Number: 2.52, P-value: 0.0000, Result: Reject\n",
      "2008 VaR Exceptions: 12 Expected Number: 2.52, P-value: 0.0000, Result: Reject\n",
      "Model: VaRt\n",
      "2007 VaR Exceptions: 27 Expected Number: 2.52, P-value: 0.0000, Result: Reject\n",
      "2008 VaR Exceptions: 9 Expected Number: 2.52, P-value: 0.0011, Result: Reject\n",
      "Model: VaRPot\n",
      "2007 VaR Exceptions: 5 Expected Number: 2.52, P-value: 0.1105, Result: Cannot Reject\n",
      "2008 VaR Exceptions: 3 Expected Number: 2.52, P-value: 0.4620, Result: Cannot Reject\n"
     ]
    }
   ],
   "source": [
    "#Kupiec\n",
    "models = ['VaRBHS', 'VaREWMA', 'VaRn', 'VaRt', 'VaRPot']\n",
    "alpha = 0.01\n",
    "\n",
    "for model in models:\n",
    "    n_violations_2007 = (data_2007['losses'] > data_2007[model]).sum()\n",
    "    n_violations_2008 = (data_2008['losses'] > data_2008[model]).sum()\n",
    "    \n",
    "    expected_violations_2007 = alpha * len(data_2007)\n",
    "    expected_violations_2008 = alpha * len(data_2008)\n",
    "    \n",
    "    pValKupiec_2007 = 1 - stats.binom.cdf(n_violations_2007 - 1, len(data_2007), alpha)\n",
    "    pValKupiec_2008 = 1 - stats.binom.cdf(n_violations_2008 - 1, len(data_2008), alpha)\n",
    "    \n",
    "    \n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"2007 VaR Exceptions: {n_violations_2007} Expected Number: {expected_violations_2007:.2f}, P-value: {pValKupiec_2007:.4f}, Result: {'Reject' if pValKupiec_2007 < alpha else 'Cannot Reject'}\")\n",
    "    print(f\"2008 VaR Exceptions: {n_violations_2008} Expected Number: {expected_violations_2008:.2f}, P-value: {pValKupiec_2008:.4f}, Result: {'Reject' if pValKupiec_2008 < alpha else 'Cannot Reject'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "82874ddf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amber Zone Critical Value: 5.0\n",
      "Red Zone Critical Value: 10.0\n",
      "Model: VaRBHS\n",
      "2007 Color Zone: Red\n",
      "2008 Color Zone: Amber\n",
      "Model: VaREWMA\n",
      "2007 Color Zone: Green\n",
      "2008 Color Zone: Amber\n",
      "Model: VaRn\n",
      "2007 Color Zone: Red\n",
      "2008 Color Zone: Red\n",
      "Model: VaRt\n",
      "2007 Color Zone: Red\n",
      "2008 Color Zone: Amber\n",
      "Model: VaRPot\n",
      "2007 Color Zone: Green\n",
      "2008 Color Zone: Green\n"
     ]
    }
   ],
   "source": [
    "#Basel Traffic Light\n",
    "#Focuses on one-sided Kupiec test\n",
    "critical_value_amber = stats.binom.ppf(0.95, len(data_2007), alpha)\n",
    "critical_value_red = stats.binom.ppf(0.9999, len(data_2008), alpha)\n",
    "\n",
    "print(f\"Amber Zone Critical Value: {critical_value_amber}\")\n",
    "print(f\"Red Zone Critical Value: {critical_value_red}\")\n",
    "\n",
    "for model in models:\n",
    "    n_violations_2007 = (data_2007['losses'] > data_2007[model]).sum()\n",
    "    n_violations_2008 = (data_2008['losses'] > data_2008[model]).sum()\n",
    "    \n",
    "    if n_violations_2007 <= critical_value_amber:\n",
    "        color_zone_2007 = \"Green\"\n",
    "    elif critical_value_amber < n_violations_2007 <= critical_value_red:\n",
    "        color_zone_2007 = \"Amber\"\n",
    "    else:\n",
    "        color_zone_2007 = \"Red\"\n",
    "    \n",
    "    if n_violations_2008 <= critical_value_amber:\n",
    "        color_zone_2008 = \"Green\"\n",
    "    elif critical_value_amber < n_violations_2008 <= critical_value_red:\n",
    "        color_zone_2008 = \"Amber\"\n",
    "    else:\n",
    "        color_zone_2008 = \"Red\"\n",
    "    \n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"2007 Color Zone: {color_zone_2007}\")\n",
    "    print(f\"2008 Color Zone: {color_zone_2008}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1368e5d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: VaRBHS\n",
      "2007 LR_ind test statistic: 1.3087, P-value: 0.2526, Result: Cannot Reject \n",
      "2008 LR_ind test statistic: 6.8231, P-value: 0.0090, Result: Reject \n",
      "Model: VaREWMA\n",
      "2007 LR_ind test statistic: 9.9663, P-value: 0.0016, Result: Reject \n",
      "2008 LR_ind test statistic: 8.2158, P-value: 0.0042, Result: Reject \n",
      "Model: VaRn\n",
      "2007 LR_ind test statistic: 5.5850, P-value: 0.0181, Result: Cannot Reject \n",
      "2008 LR_ind test statistic: 6.3087, P-value: 0.0120, Result: Cannot Reject \n",
      "Model: VaRt\n",
      "2007 LR_ind test statistic: 5.9135, P-value: 0.0150, Result: Cannot Reject \n",
      "2008 LR_ind test statistic: 4.7217, P-value: 0.0298, Result: Cannot Reject \n",
      "Model: VaRPot\n",
      "2007 LR_ind test statistic: 0.2434, P-value: 0.6217, Result: Cannot Reject \n",
      "2008 LR_ind test statistic: 5.4650, P-value: 0.0194, Result: Cannot Reject \n"
     ]
    }
   ],
   "source": [
    "# Christoffersen Independence Test\n",
    "for model in models:\n",
    "    #2007\n",
    "    data_2007['violations_' + model + '_2007'] = np.where(data_2007['losses'] > data_2007[model], 1, 0)\n",
    "    n1_2007 = (data_2007['violations_' + model + '_2007'] == 1).sum()\n",
    "    n0_2007 = len(data_2007) - n1_2007\n",
    "    n11_2007 = 0\n",
    "    n00_2007 = 0\n",
    "    n01_2007 = 0\n",
    "    n10_2007 = 0\n",
    "    \n",
    "    for r in range(0, len(data_2007) - 1):\n",
    "        if data_2007['violations_' + model + '_2007'][r] == 1 and data_2007['violations_' + model + '_2007'][r + 1] == 1:\n",
    "            n11_2007 += 1\n",
    "        elif data_2007['violations_' + model + '_2007'][r] == 0 and data_2007['violations_' + model + '_2007'][r + 1] == 0:\n",
    "            n00_2007 += 1\n",
    "        elif data_2007['violations_' + model + '_2007'][r] == 0 and data_2007['violations_' + model + '_2007'][r + 1] == 1:\n",
    "            n01_2007 += 1\n",
    "        elif data_2007['violations_' + model + '_2007'][r] == 1 and data_2007['violations_' + model + '_2007'][r + 1] == 0:\n",
    "            n10_2007 += 1\n",
    "    \n",
    "    pi00_2007 = n00_2007 / (n00_2007 + n01_2007)\n",
    "    pi01_2007 = n01_2007 / (n00_2007 + n01_2007)\n",
    "    pi10_2007 = n10_2007 / (n10_2007 + n11_2007)\n",
    "    pi11_2007 = n11_2007 / (n10_2007 + n11_2007)\n",
    "    pi0_2007 = n0_2007 / (n0_2007 + n1_2007)\n",
    "    pi1_2007 = n1_2007 / (n0_2007 + n1_2007)\n",
    "\n",
    "    lnNull_2007 = np.log(pi0_2007**n0_2007*pi1_2007**n1_2007)\n",
    "    lnAlt_2007 = np.log(pi00_2007**n00_2007*pi01_2007**n01_2007*pi10_2007**n10_2007*pi11_2007**n11_2007)\n",
    "    LRind_2007 = -2 * (lnNull_2007 - lnAlt_2007)\n",
    "    pVal_2007 = 1 - stats.chi2.cdf(LRind_2007, 1)       \n",
    "    \n",
    "    #2008\n",
    "    data_2008['violations_' + model + '_2008'] = np.where(data_2008['losses'] > data_2008[model], 1, 0)\n",
    "    n1_2008 = (data_2008['violations_' + model + '_2008'] == 1).sum()\n",
    "    n0_2008 = len(data_2008) - n1_2008\n",
    "    n11_2008 = 0\n",
    "    n00_2008 = 0\n",
    "    n01_2008 = 0\n",
    "    n10_2008 = 0\n",
    "        \n",
    "    for r in range(0, len(data_2008) - 1):\n",
    "        if data_2008['violations_' + model + '_2008'][r] == 1 and data_2008['violations_' + model + '_2008'][r + 1] == 1:\n",
    "            n11_2008 += 1\n",
    "        elif data_2008['violations_' + model + '_2008'][r] == 0 and data_2008['violations_' + model + '_2008'][r + 1] == 0:\n",
    "            n00_2008 += 1\n",
    "        elif data_2008['violations_' + model + '_2008'][r] == 0 and data_2008['violations_' + model + '_2008'][r + 1] == 1:\n",
    "            n01_2008 += 1\n",
    "        elif data_2008['violations_' + model + '_2008'][r] == 1 and data_2008['violations_' + model + '_2008'][r + 1] == 0:\n",
    "            n10_2008 += 1\n",
    "    \n",
    "    pi00_2008 = n00_2008 / (n00_2008 + n01_2008)\n",
    "    pi01_2008 = n01_2008 / (n00_2008 + n01_2008)\n",
    "    pi10_2008 = n10_2008 / (n10_2008 + n11_2008)\n",
    "    pi11_2008 = n11_2008 / (n10_2008 + n11_2008)\n",
    "    pi0_2008 = n0_2008 / (n0_2008 + n1_2008)\n",
    "    pi1_2008 = n1_2008 / (n0_2008 + n1_2008)\n",
    "\n",
    "    lnNull_2008 = np.log(pi0_2008**n0_2008*pi1_2008**n1_2008)\n",
    "    lnAlt_2008 = np.log(pi00_2008**n00_2008*pi01_2008**n01_2008*pi10_2008**n10_2008*pi11_2008**n11_2008)\n",
    "    LRind_2008 = -2 * (lnNull_2008 - lnAlt_2008)\n",
    "    pVal_2008 = 1 - stats.chi2.cdf(LRind_2008, 1)\n",
    "    \n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"2007 LR_ind test statistic: {LRind_2007:.4f}, P-value: {pVal_2007:.4f}, Result: {'Reject' if pVal_2007 < alpha else 'Cannot Reject'} \")\n",
    "    print(f\"2008 LR_ind test statistic: {LRind_2008:.4f}, P-value: {pVal_2008:.4f}, Result: {'Reject' if pVal_2008 < alpha else 'Cannot Reject'} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b9021a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ESEWMA\n",
      "2007 ES Acerbi-Szekely test statistic: -0.4661 \n",
      "2008 ES Acerbi-Szekely test statistic: -1.0487 \n",
      "Model: ESn\n",
      "2007 ES Acerbi-Szekely test statistic: -18.4620 \n",
      "2008 ES Acerbi-Szekely test statistic: -6.5774 \n",
      "Model: ESt\n",
      "2007 ES Acerbi-Szekely test statistic: -1.3059 \n",
      "2008 ES Acerbi-Szekely test statistic: 0.1105 \n",
      "Model: ESPot\n",
      "2007 ES Acerbi-Szekely test statistic: -1.1670 \n",
      "2008 ES Acerbi-Szekely test statistic: -0.5021 \n"
     ]
    }
   ],
   "source": [
    "#ES Acerbi-Szekely (2015) test \n",
    "models = ['EWMA', 'n', 't', 'Pot']\n",
    "alpha=0.01\n",
    "for model in models:\n",
    "\n",
    "    z_2007 = (-1)*(1/(len(data_2007)*alpha))*sum((data_2007['violations_' + 'VaR' + model + '_2007'] * data_2007['losses'] / data_2007['ES'+ model])) + 1\n",
    "    z_2008 = (-1)*(1/(len(data_2008)*alpha))*sum((data_2008['violations_' + 'VaR' + model + '_2008'] * data_2008['losses'] / data_2008['ES'+ model])) + 1\n",
    "    #alpha=0.01, so not use (1-alpha) to get estimated number of violation\n",
    "    \n",
    "    print(f\"Model: ES{model}\")\n",
    "    print(f\"2007 ES Acerbi-Szekely test statistic: {z_2007:.4f} \")\n",
    "    print(f\"2008 ES Acerbi-Szekely test statistic: {z_2008:.4f} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9793e985",
   "metadata": {},
   "source": [
    "z closer to 0, the better the model,\n",
    "So ESEWMA, ESt and ESPot seem a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c72ce4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical Values {3: -4.540702858698419, 5: -3.3649299989072747, 10: -2.763769457447889, 'N(0,1)': -2.3263478740408408}\n"
     ]
    }
   ],
   "source": [
    "#Simulate test statistics\n",
    "degrees_of_freedom = [3, 5, 10, 'N(0,1)']\n",
    "alpha = 0.01\n",
    "\n",
    "critical_values_es = {}\n",
    "\n",
    "for df in degrees_of_freedom:\n",
    "    if df == 'N(0,1)':\n",
    "        es_critical = stats.norm.ppf(1 - alpha)\n",
    "    else:\n",
    "        es_critical = stats.t.ppf(1 - alpha, df)\n",
    "    \n",
    "    critical_values_es[df] = -es_critical\n",
    "\n",
    "print(f\"Critical Values {critical_values_es}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be14505",
   "metadata": {},
   "source": [
    "ESEWMA and ESn degree of freedom is infinite, so use critical value of -2.33;\n",
    "ESt and ESPot degree of freedom near 3, so use critical value of -4.54;\n",
    "So in the ES Acerbi-Szekely test, we cannot reject model ESEWMA, ESt and ESPot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367aa7ef",
   "metadata": {},
   "source": [
    "## Final Result:\n",
    "a.\n",
    "During the Kupiec test, BHS has bad performance in 2007 and good performance in 2008;\n",
    "During the Christoffersen Independence Test, BHS has good performance in 2007 and bad performance in 2008;\n",
    "Other models show the same performance for both 2007 and 2008;\n",
    "So we can summarize for the VaR model: \n",
    "For VaRBHS, the method perform bad for both 2007 and 2008;\n",
    "For VaREWMA, the method perform well for both 2007 and 2008;\n",
    "For VaRn, the method perform bad for both 2007 and 2008;\n",
    "For VaRt, the method perform bad for both 2007 and 2008;\n",
    "For VaRPot, the method perform well for both 2007 and 2008;\n",
    "\n",
    "We can also summarize for the ES model: \n",
    "For ESEWMA, the method perform well for both 2007 and 2008;\n",
    "For ESn, the method perform bad for both 2007 and 2008;\n",
    "For ESt, the method perform well for both 2007 and 2008;\n",
    "For ESPot, the method perform well for both 2007 and 2008;\n",
    "\n",
    "So generally, the same methods perform well or not so well for different sample years.\n",
    "\n",
    "\n",
    "\n",
    "b.\n",
    "During 2007 and 2008, the risk was increasingly high due to the financial crisis, so intuitively we should consider extreme situations, normal distribution may not be suitable.\n",
    "\n",
    "We choose Pot method for VaR to evaluate extreme situations, as VaR focuses on the maximum potential loss.\n",
    "\n",
    "We choose t distribution method for ES to focus on a fatter tail of losses, as ES is about the average loss that would be incurred if the portfolio falls above the VaR threshold, and choosing a different method compared with the VaR method of Pot can help us to gain a more comprehensive understanding of the risk. We do not choose the EMWA method because the volatility in 2007 and 2008 is not in cluster mode, and exhibits significant short-term variations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b1d5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
